# RAG Indexing Pipeline - Testing Guide

## Implementation Complete ‚úÖ

All components have been successfully implemented:

1. **OllamaClient** (`api/ollama/OllamaClient.kt`) - HTTP client for Ollama embeddings API
2. **OllamaModels** (`api/ollama/OllamaModels.kt`) - Request/response data models
3. **RagModels** (`core/rag/RagModels.kt`) - Chunk, EmbeddedChunk, RagIndex models
4. **DocumentChunker** (`core/rag/DocumentChunker.kt`) - Fixed-size text chunking with overlap
5. **RagStorage** (`core/rag/RagStorage.kt`) - JSON serialization for index storage
6. **IndexingService** (`core/rag/IndexingService.kt`) - Pipeline orchestrator
7. **CliFrontend** - Added `/index` command handler
8. **Main.kt** - Wired up all components

## How to Test

### Prerequisites
1. Ensure Ollama is running: `http://127.0.0.1:11434`
2. Ensure the model is available. Test with:
```bash
curl -X POST http://localhost:11434/api/embed \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mxbai-embed-large",
    "input": ["test"]
  }'
```

### Running the Indexing Pipeline

1. Start the application:
```bash
./gradlew run
```

2. When the prompt appears, type:
```
/index
```

3. You should see progress messages like:
```
=== –°–æ–∑–¥–∞–Ω–∏–µ RAG –∏–Ω–¥–µ–∫—Å–∞ ===
üîç Scanning for .md files...
üìö Found 4 documents: README.md, README.md.1, README.md.2, README.md.3
‚úÇÔ∏è Splitting documents into chunks...
üìù Generated X chunks
üß† Generating embeddings with model: mxbai-embed-large...
   Processing batch 1/Y (10 chunks)...
   Processing batch 2/Y (10 chunks)...
‚úÖ Generated X embeddings
üíæ Saving index...
‚úÖ Index saved successfully! Total chunks: X
–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: dataForRag/indexed/index.json
========================
```

### Verifying the Results

1. Check that the index file was created:
```bash
ls -lh dataForRag/indexed/index.json
```

2. Inspect the index structure:
```bash
head -n 50 dataForRag/indexed/index.json
```

Expected structure:
```json
{
  "model": "mxbai-embed-large",
  "createdAt": "2025-12-22T...",
  "chunks": [
    {
      "text": "chunk text...",
      "source": "README.md",
      "position": 0,
      "embedding": [0.123, -0.456, ...]
    },
    ...
  ]
}
```

3. Verify embedding dimensions (should be 1024 for mxbai-embed-large):
```bash
cat dataForRag/indexed/index.json | jq '.chunks[0].embedding | length'
```

## Configuration

Default settings (in IndexingService):
- **Model**: `mxbai-embed-large`
- **Chunk size**: 500 characters
- **Overlap**: 50 characters
- **Batch size**: 10 chunks per API call
- **Source directory**: `dataForRag/raw`
- **Output directory**: `dataForRag/indexed`

## Error Handling

If Ollama is not running, you'll see:
```
‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: Failed to generate embeddings from Ollama: ...
–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:
  - Ollama –∑–∞–ø—É—â–µ–Ω–∞ (http://127.0.0.1:11434)
  - –ú–æ–¥–µ–ª—å mxbai-embed-large –¥–æ—Å—Ç—É–ø–Ω–∞
  - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è dataForRag/raw —Å–æ–¥–µ—Ä–∂–∏—Ç .md —Ñ–∞–π–ª—ã
```

## Architecture

```
User types /index
    ‚Üì
CliFrontend.handleIndexCommand()
    ‚Üì
IndexingService.buildIndex()
    ‚Üì
1. Load .md files from dataForRag/raw
2. DocumentChunker splits texts into chunks (500 chars, 50 overlap)
3. OllamaClient generates embeddings (batches of 10)
4. RagStorage saves index as JSON to dataForRag/indexed/
```

## Next Steps

After indexing is complete, you can:
1. Load the index using `RagStorage.loadIndex()`
2. Implement similarity search using cosine similarity
3. Integrate with the conversation manager for RAG-based responses
4. Build a retrieval system that finds relevant chunks for user queries

